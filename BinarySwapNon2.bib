% Encoding: UTF-8

@Article{Molnar1994,
  author  = {Steven Molnar and Michael Cox and David Ellsworth and Henry Fuchs},
  title   = {A Sorting Classification of Parallel Rendering},
  journal = {IEEE Computer Graphics and Applications},
  year    = {1994},
  volume  = {14},
  number  = {4},
  pages   = {23--32},
  month   = {July},
  comment = {The seminal paper on the sorting classification for parallel rendering (sort-first, sort-middle, sort-last).},
}

@Article{Wylie2001,
  author  = {Brian Wylie and Constantine Pavlakos and Vasily Lewis and Kenneth Moreland},
  title   = {Scalable Rendering on {PC} Clusters},
  journal = {IEEE Computer Graphics and Applications},
  year    = {2001},
  volume  = {21},
  number  = {4},
  pages   = {62--70},
  month   = {July/August},
  comment = {Early article demonstrating the good scaling performance of sort-last parallel rendering.  (Does not really compare to other parallel rendering methods, although those are known to scale poorly.  You can use [Mueller1995] to argue poor scaling of sort-first.)},
}

@Article{Childs2010,
  author   = {Hank Childs and David Pugmire and Sean Ahern and Brad Whitlock and Mark Howison and Prabhat and Gunther H. Weber and E. Wes Bethel},
  title    = {Extreme Scaling of Production Visualization Software on Diverse Architectures},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2010},
  volume   = {30},
  number   = {3},
  pages    = {22--31},
  month    = {May/June},
  abstract = {This article presents the results of experiments studying how the pure-parallelism paradigm scales to massive data sets, including 16,000 or more cores on trillion-cell meshes, the largest data sets published to date in the visualization literature. The findings on scaling characteristics and bottlenecks contribute to understanding how pure parallelism will perform in the future.},
  comment  = {Explores the scaling of visualization pipelines (VisIt specifically) on full petascale machines.},
  doi      = {10.1109/MCG.2010.51},
  url      = {http://dx.doi.org/10.1109/MCG.2010.51},
}

@InProceedings{Moreland2011:SC,
  author    = {Kenneth Moreland and Wesley Kendall and Tom Peterka and Jian Huang},
  title     = {An Image Compositing Solution at Scale},
  booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11)},
  year      = {2011},
  month     = {November},
  comment   = {A compendium of optimizations for sort-last image-compositing parallel rendering including  Radix-k compositing order, image compression, image interlacing, non-powers-of-two binary swap (telescoping), and improved image collection.  All the techniques are collected in the IceT parallel rendering library and scalability is shown on a petascale machine.},
  doi       = {10.1145/2063384.2063417},
  url       = {http://dx.doi.org/10.1145/2063384.2063417},
}

@InProceedings{Peterka2009:ICPP,
  author    = {Tom Peterka and Hongfeng Yu and Robert Ross and Kwan-Liu Ma and Rob Latham},
  title     = {End-to-End Study of Parallel Volume Rendering on the {IBM Blue Gene/P}},
  booktitle = {Proceedings of ICPP '09},
  year      = {2009},
  pages     = {566--573},
  month     = {September},
  comment   = {Measures very large scale parallel volume rendering.  Shows evidence of I/O dominating visualization/rendering.},
  doi       = {10.1109/ICPP.2009.27},
  url       = {http://dx.doi.org/10.1109/ICPP.2009.27},
}

@InBook{Peterka2013,
  chapter   = {Parallel Image Compositing Methods},
  pages     = {71--89},
  title     = {High Performance Visualization: Enabling Extreme Scale Insight},
  publisher = {CRC Press},
  year      = {2013},
  author    = {Tom Peterka and Kwan-Liu Ma},
  isbn      = {978-1-4398-7572-8},
  /*editor  = {E. Wes Bethel and Hank Childs and Charles Hansen},
  comment   = {A basic overview of sort-last parallel image compositing methods. Includes discussion of direct-send, tree, binary-swap, 2-3 swap, and radix-k.},
}

@Article{SkyBridge,
  author  = {Neal Singer},
  title   = {Sandia turns on Sky Bridge supercomputer},
  journal = {Sandia Lab News},
  year    = {2014},
  month   = {December 12},
  note    = {\url{http://www.sandia.gov/news/publications/labnews/archive/14-12-12.html#3}},
  comment = {A simple source for the Sky Bridge computer. Not the best publication in the world, but there are few publications on it.},
  url     = {http://www.sandia.gov/news/publications/labnews/archive/14-12-12.html#3},
}

@InProceedings{23Swap,
  author    = {Hongfeng Yu and Chaoli Wang and Kwan-Liu Ma},
  title     = {Massively Parallel Volume Rendering Using 2-3 Swap Image Compositing},
  booktitle = {Proceedings of the 2008 ACM/IEEE Conference on Supercomputing},
  year      = {2008},
  month     = {November},
  comment   = {A varient of binary swap that handles non-powers of two.  Works by allowing groups of either two or three, possibly at the same time.  If both are used, the images are re-partitioned to sixths and redistributed/composited as necessary.},
  doi       = {10.1109/SC.2008.5219060},
}

@InProceedings{Ahrens1998,
  author    = {James Ahrens and James Painter},
  title     = {Efficient Sort-Last Rendering Using Compression-Based Image Compositing},
  booktitle = {Second Eurographics Workshop on Parallel Graphics and Visualization},
  year      = {1998},
  month     = {September},
  comment   = {Early suggestion to use run lengths in image compositing. (There is a reference to an even earlier suggestion.)},
}

@InProceedings{Moreland2001,
  author    = {Kenneth Moreland and Brian Wylie and Constantine Pavlakos},
  title     = {Sort-Last Parallel Rendering for Viewing Extremely Large Data Sets on Tile Displays},
  booktitle = {Proceedings of the IEEE 2001 Symposium on Parallel and Large-Data Visualization and Graphics},
  year      = {2001},
  pages     = {85--92},
  month     = {October},
  comment   = {Initial publication of the tiled display sort-last image compositing algorithms that form the basis for IceT.},
}

@Misc{OpenSWR,
  author       = {Bruce Cherniak},
  title        = {{OpenSWR}: A Scalable High-Performance Software Rasterizer for Scivis},
  howpublished = {Presentation, {\it Intel HPC Developer Conference}},
  month        = nov,
  year         = {1015},
  comment      = {Introduces the OpenSWR parallel rasterizer for rendering.

This is not a great source. It is not a real publication, it is just a presentation. In fact, it is a presentation made by an Intel employee at an event organized by Intel, so don't expect any external peer review here. However, the OpenSWR team does not seem to be interested in publishing, and this is the best that I could find.},
  url          = {http://openswr.org/talks/SC15DevConf-OpenSWR.pdf},
}

@InProceedings{Larsen2015:RayTrace,
  author    = {Matthew Larsen and Jeremy S. Meredith and Paul A. Navratil and Hank Childs},
  title     = {Ray Tracing Within a Data Parallel Framework},
  booktitle = {IEEE Pacific Visualization Symposium (PacificVis)},
  year      = {2015},
  pages     = {279--286},
  month     = {April},
  abstract  = {Current architectural trends on supercomputers have dramatic increases in the number of cores and available computational power per die, but this power is increasingly difficult for programmers to harness effectively. High-level language constructs can simplify programming many-core devices, but this ease comes with a potential loss of processing power, particularly for cross-platform constructs. Recently, scientific visualization packages have embraced language constructs centering around data parallelism, with familiar operators such as map, reduce, gather, and scatter. Complete adoption of data parallelism will require that central visualization algorithms be revisited, and expressed in this new paradigm while preserving both functionality and performance. This investment has a large potential payoff: portable performance in software bases that can span over the many architectures that scientific visualization applications run on. With this work, we present a method for ray tracing consisting of entirely of data parallel primitives. Given the extreme computational power on nodes now prevalent on supercomputers, we believe that ray tracing can supplant rasterization as the work-horse graphics solution for scientific visualization. Our ray tracing method is relatively efficient, and we describe its performance with a series of tests, and also compare to leading-edge ray tracers that are optimized for specific platforms. We find that our data parallel approach leads to results that are acceptable for many scientific visualization use cases, with the key benefit of providing a single code base that can run on many architectures.},
  comment   = {An experiment where a ray tracer was written using a system of data-parallel primitives (specifically in EAVL although later work moved to VTK-m). The EAVL ray tracer was compared with NVIDIA's in-house solution designed specifically for CUDA (OptiX) and the Intel in-house solution for Xeon (Embree). The portable data-parallel primitive implementation got within a factor of two of each.},
  doi       = {10.1109/PACIFICVIS.2015.7156388},
  url       = {http://dx.doi.org/10.1109/PACIFICVIS.2015.7156388},
}

@Article{Moreland2016:VTKm,
  author   = {Kenneth Moreland and Christopher Sewell and William Usher and Li-Ta Lo and Jeremy Meredith and David Pugmire and James Kress and Hendrik Schroots and Kwan-Liu Ma and Hank Childs and Matthew Larsen and Chun-Ming Chen and Robert Maynard and Berk Geveci},
  title    = {{VTK-m}: Accelerating the Visualization Toolkit for Massively Threaded Architectures},
  journal  = {IEEE Computer Graphics and Applications},
  year     = {2016},
  volume   = {36},
  number   = {3},
  pages    = {48--58},
  month    = {May/June},
  abstract = {One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.},
  comment  = {The first comprehensive publication for VTK-m.},
  doi      = {10.1109/MCG.2016.48},
  url      = {http://dx.doi.org/10.1109/MCG.2016.48},
}

@Article{Wald2014,
  author   = {Ingo Wald and Sven Woop and Carsten Benthin and Gregory S. Johnson and Manfred Ernst},
  title    = {Embree: A Kernel Framework for Efficient {CPU} Ray Tracing},
  journal  = {ACM Transactions on Graphics (TOG)},
  year     = {2014},
  volume   = {33},
  number   = {4},
  month    = jul,
  abstract = {We describe Embree, an open source ray tracing framework for x86 CPUs. Embree is explicitly designed to achieve high performance in professional rendering environments in which complex geometry and incoherent ray distributions are common. Embree consists of a set of low-level kernels that maximize utilization of modern CPU architectures, and an API which enables these kernels to be used in existing renderers with minimal programmer effort. In this paper, we describe the design goals and software architecture of Embree, and show that for secondary rays in particular, the performance of Embree is competitive with (and often higher than) existing state-of-the-art methods on CPUs and GPUs.},
  comment  = {The paper to use to reference Intel's Embree ray casting software. Probably also the best reference for OSPRay (or at least the performance of it). Although OSPRay is never mentioned in the paper, Embree is the underlying ray casting engine for OSPRay.},
  doi      = {10.1145/2601097.2601199},
}

@Article{Knoll2014,
  author   = {Aaron Knoll and Ingo Wald and Paul Navratil and Anne Bowen and Khairi Reda and Michael E. Papka and Kelly Gaither},
  title    = {{RBF} Volume Ray Casting on Multicore and Manycore {CPUs}},
  journal  = {Computer Graphics Forum (Proceedings of EuroVis)},
  year     = {2014},
  volume   = {33},
  number   = {3},
  pages    = {71--80},
  month    = jun,
  abstract = {Modern supercomputers enable increasingly large N‐body simulations using unstructured point data. The structures implied by these points can be reconstructed implicitly. Direct volume rendering of radial basis function (RBF) kernels in domain‐space offers flexible classification and robust feature reconstruction, but achieving performant RBF volume rendering remains a challenge for existing methods on both CPUs and accelerators. In this paper, we present a fast CPU method for direct volume rendering of particle data with RBF kernels. We propose a novel two‐pass algorithm: first sampling the RBF field using coherent bounding hierarchy traversal, then subsequently integrating samples along ray segments. Our approach performs interactively for a range of data sets from molecular dynamics and astrophysics up to 82 million particles. It does not rely on level of detail or subsampling, and offers better reconstruction quality than structured volume rendering of the same data, exhibiting comparable performance and requiring no additional preprocessing or memory footprint other than the BVH. Lastly, our technique enables multi‐field, multi‐material classification of particle data, providing better insight and analysis. },
  comment  = {Documents the high performance volume rendering work lead by Aaron Knoll.},
  doi      = {https://doi.org/10.1111/cgf.12363},
}

@InProceedings{BinarySwap1,
  author    = {Kwan-Liu Ma and James S. Painter and Charles D. Hansen and Michael F. Krogh},
  title     = {A Data Distributed, Parallel Algorithm for Ray-Traced Volume Rendering},
  booktitle = {Proceedings of the 1993 Symposium on Parallel Rendering},
  year      = {1993},
  pages     = {15--22},
  comment   = {One of the original binary-swap papers.},
  doi       = {10.1145/166181.166183},
  url       = {http://dx.doi.org/10.1145/166181.166183},
}

@InProceedings{Nonaka2015,
  author    = {Jorji Nonaka and Kenji Ono and Masahiro Fujita},
  title     = {234 Scheduling of 3-2 and 2-1 Eliminations for Parallel Image Compositing Using Non-Power-of-Two Number of Processes},
  booktitle = {International Conference on High Performance Computing \& Simulation (HPCS)},
  year      = {2015},
  month     = jul,
  abstract  = {Binary-Swap is a parallel image compositing algorithm based on recursive vector halving and distance doubling, and works efficiently when the number of processes is exactly a power-of-two (2n). Several power-of-two converting approaches for Binary-Swap have been proposed. Among them, the Telescope method, based on the Binary Blocks algorithm, has been shown as the most promising approach. The Telescope method decomposes an entire set of processes into blocks of power-of-two size and merges the smaller blocks into larger blocks in stepwise fashion. This block merging process corresponds to the communication and computational overhead of the conversion, and since it can only merge one block per stage, it becomes inefficient as the number of binary blocks increases. In this paper, we focus on a single-stage conversion method using the 3-2 and 2-1 elimination approaches. The original scheduling method, proposed by Rabenseifner et al., is limited to an odd number of processes since it always schedules a single 3-2 elimination per conversion. Taking into consideration that the 3-2 elimination can be optimized on modern HPC systems, which can overlap the communication and computation, we propose 234 Scheduling for scheduling multiple 3-2 eliminations per conversion. The multiple 3-2 elimination scheduling enlarges the application range by enabling its use on an even number of processes. We evaluated 234 Scheduling applied to Binary-Swap on the K computer, which is a modern parallel HPC system, and confirmed its effectiveness.},
  comment   = {A compositing algorithm that is similar to [Rabenseifner2004] and the remainder algorithm I am thinking about. (Image compositing for sort-last parallel rendering.) The difference is that where Rabenseifner and I want to do a "3-2 reduction" on a single group of 3 processes (to deal with iterations that have odd groups) so that you drop at most one process per piece, Nonaka makes as many 3 groups as possible (each dropping 1 process) and one group of 4, which drops 2 processes (a 4-2 reduction). It's not clear why the algorithm is dropping so many processes per iteration. I think they missunderstand Roabenseifner's algorithm and assume that all groups are 3-2 reduction or 2-1 reduction (or 4-2 reduction).},
  doi       = {10.1109/HPCSim.2015.7237071},
}

@Article{Nonaka2018,
  author   = {Jorji Nonaka and Kenji Ono and Masahiro Fujita},
  title    = {{234Compositor}: A flexible parallel image compositing framework for massively parallel visualization environments},
  journal  = {Future Generation Computer Systems},
  year     = {2018},
  volume   = {82},
  pages    = {647--655},
  abstract = {Leading-edge HPC systems have already been generating a vast amount of time-varying complex data sets, and future-generation HPC systems are expected to produce much higher amounts of such data, thus making their visualization and analysis a much more challenging task. In such scenario, the In-situ visualization approach, where the same HPC system is used for both numerical simulation and visualization, is expected to become more a necessity than an option. On massively parallel environments, the Sort-last approach, which requires final image compositing, has become the de facto standard for parallel rendering. In this work, we present the 234Compositor, a scalable and flexible parallel image compositor framework for massively parallel rendering applications. It is composed of a single-stage power-of-two conversion mechanism based on 234 Scheduling of 3-2 and 2-1 Eliminations, and a final image gathering mechanism based on Data Padding and MPI Rank Reordering for enabling the use of MPI_Gather collective operation. In addition, the hybrid MPI/OpenMP parallelism can also be applied to take advantage of current multi-node, multi-core architecture of modern HPC systems. We confirmed the scalability of the proposed approach by evaluating a Binary-Swap implementation of 234Compositor on the K computer, a Japanese leading-edge supercomputer installed at RIKEN AICS. We also evaluated an integration with HIVE (Heterogeneously Integrated Visual-analytic Environment) in order to verify a real-world usage. From the encouraging scalability results, we expect that this approach can also be useful even on the next-generation HPC systems which may demand higher level of parallelism.},
  comment  = {An extension of [Nonaka2015]. The addition, as I understand it, is that it does a rank reordering and image padding to be able to do an MPI_Gather instead of an MPI_Gatherv, the latter of which is shown to be significantly slower.},
  doi      = {10.1016/j.future.2017.02.011},
}

@InProceedings{Rabenseifner2004,
  author    = {Rolf Rabenseifner and Jesper Larsson Tr\"{a}ff},
  title     = {More Efficient Reduction Algorithms for Non-Power-of-Two Number of Processors in Message-Passing Parallel Systems},
  booktitle = {Recent Advances in Parallel Virtual Machine and Message Passing Interface (EuroPVM/MPI)},
  year      = {2004},
  volume    = {3241},
  series    = {Lecture Notes in Computer Science},
  pages     = {36--46},
  abstract  = {We present improved algorithms for global reduction operations for message-passing systems. Each of p processors has a vector of m data items, and we want to compute the element-wise “sum” under a given, associative function of the p vectors. The result, which is also a vector of m items, is to be stored at either a given root processor (MPI_Reduce), or all p processors (MPI_Allreduce). A further constraint is that for each data item and each processor the result must be computed in the same order, and with the same bracketing. Both problems can be solved in O(m+log2 p) communication and computation time. Such reduction operations are part of MPI (the Message Passing Interface), and the algorithms presented here achieve significant improvements over currently implemented algorithms for the important case where p is not a power of 2. Our algorithm requires ⌈log2 p⌉ + 1 rounds – one round off from optimal – for small vectors. For large vectors twice the number of rounds is needed, but the communication and computation time is less than 3mβ and 3/2mγ, respectively, an improvement from 4mβ and 2mγ achieved by previous algorithms (with the message transfer time modeled as α + mβ, and reduction-operation execution time as mγ). For p=3× 2 n and p=9× 2 n and small m ≤ b for some threshold b, and p=q 2 n with small q, our algorithm achieves the optimal ⌈log2 p⌉ number of rounds.},
  comment   = {A paper that, among other things, presents a general reduction that is essentially the same as the "remainder" algorithm that I have been exploring. This paper calls the technique 3-to-2 process reduction. One difference between Rabenseifner's approach and mine is that in my approach one process divides its data and sends each half to a different process and drop out whereas Rabenseifner's approach has each process receive half and one process also sends half and then drops out.},
  doi       = {10.1007/978-3-540-30218-6_13},
}

@Comment{jabref-meta: databaseType:bibtex;}
